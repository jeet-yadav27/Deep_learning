{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwoUP/20OPH/BXkTpNsErd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeet-yadav27/Deep_learning/blob/main/8_How_to_Develop_CNN_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e32tpi0hUhHJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CQHZMpjIUhq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Oa480x3ZUqHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gNGqfvWHUxkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Univariate CNN Models\n",
        "2. Multivariate CNN Models\n",
        "3. Multi-step CNN Models\n",
        "# 4. Multivariate Multi-step CNN Models"
      ],
      "metadata": {
        "id": "YVzdrBRZUxmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ###Univariate CNN Architecture\n",
        "\n",
        "1. Input Layer: 1D sequence of univariate time series values.\n",
        "2. Convolutional Layers: 1D convolutional filters scan the input sequence.\n",
        "3. Activation Functions: ReLU, Tanh, or others.\n",
        "4. Pooling Layers: Downsample the feature maps.\n",
        "5. Flatten Layer: Convert feature maps to 1D.\n",
        "6. Dense Layers: Output forecasted values.\n",
        "\n",
        "Key Components\n",
        "\n",
        "1. Convolutional Filters: Learn local patterns and features.\n",
        "2. Stride: Control the filter's movement along the sequence.\n",
        "3. Padding: Handle boundary effects.\n",
        "4. Dilation: Increase receptive field.\n",
        "\n",
        "Univariate CNN Advantages\n",
        "\n",
        "1. Automatic Feature Extraction: Learn relevant features.\n",
        "2. Handling Non-Linear Relationships: Non-linear activation functions.\n",
        "3. Robustness to Noise: Pooling and convolutional operations.\n",
        "\n",
        "Theoretical Considerations\n",
        "\n",
        "1. Receptive Field: Number of input values seen by each filter.\n",
        "2. Filter Size: Controls the number of learnable parameters.\n",
        "3. Number of Filters: Controls the number of feature maps.\n",
        "\n",
        "Common Architectures\n",
        "\n",
        "1. CNN-FCN (Convolutional Neural Network-Fully Convolutional Network)\n",
        "2. TCN (Temporal Convolutional Network)\n",
        "3. ResCNN (Residual Convolutional Neural Network)\n",
        "\n",
        "Training Considerations\n",
        "\n",
        "1. Loss Functions: Mean Squared Error (MSE), Mean Absolute Error (MAE).\n",
        "2. Optimization Algorithms: Adam, SGD.\n",
        "3. Regularization Techniques: Dropout, L1/L2 regularization.\n",
        "\n",
        "Real-World Applications\n",
        "\n",
        "1. Time Series Forecasting: Financial markets, weather, traffic.\n",
        "2. Anomaly Detection: Fault detection, intrusion detection.\n",
        "3. Signal Processing: Audio, image processing."
      ],
      "metadata": {
        "id": "gtG0iAupUqJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yygykuONbkSg",
        "outputId": "8203ac1f-518d-4fbe-a504-ebeb7de1f04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install tensorflow==2.13.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wNU5gYudbkUo",
        "outputId": "04ca3b3a-dbfb-424e-d495-39a6dcc2ed88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.13.0\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.13.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.11.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow==2.13.0)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (18.1.1)\n",
            "Collecting numpy<=1.24.3,>=1.22 (from tensorflow==2.13.0)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow==2.13.0)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow==2.13.0)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.4.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow==2.13.0)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n",
            "Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: typing-extensions, tensorflow-estimator, numpy, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sqlalchemy 2.0.35 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "albumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.24.3 which is incompatible.\n",
            "pydantic 2.9.2 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.23.4 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.13.0 which is incompatible.\n",
            "torch 2.4.1+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typeguard 4.3.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 keras-2.13.1 numpy-1.24.3 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 typing-extensions-4.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "keras",
                  "numpy",
                  "tensorflow"
                ]
              },
              "id": "31fcaf91e2374852ac836c09dfb210ff"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Univariate CNN example\n",
        "\n",
        "from numpy import array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "\n",
        "# Split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequence)):\n",
        "        # Find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "\n",
        "        # Check if we are beyond the sequence\n",
        "        if end_ix > len(sequence) - 1:\n",
        "            break\n",
        "\n",
        "        # Gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "\n",
        "    return array(X), array(y)\n",
        "\n",
        "# Define input sequence\n",
        "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "\n",
        "# Choose a number of time steps\n",
        "n_steps = 3\n",
        "\n",
        "# Split into samples\n",
        "X, y = split_sequence(raw_seq, n_steps)\n",
        "\n",
        "# Reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "n_features = 1\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
        "\n",
        "# Define model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Fit model\n",
        "model.fit(X, y, epochs=1000, verbose=0)\n",
        "\n",
        "# Demonstrate prediction\n",
        "x_input = array([70, 80, 90])\n",
        "x_input = x_input.reshape((1, n_steps, n_features))\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItW_rFkzUqwV",
        "outputId": "b0e9727d-37e4-419a-ee20-ca693edadad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[101.41136]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6M3YscGTdpI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second type to do Univarient TSF- Multi-headed CNN model"
      ],
      "metadata": {
        "id": "8W9rClJxdpLX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "081sPF95dyS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ere's a theoretical explanation of a Multi-Headed CNN model for Univariate Time Series Forecasting (TSF):\n",
        "\n",
        "Architecture:\n",
        "\n",
        "1. Input Layer: 1D sequence of univariate time series values.\n",
        "2. Convolutional Block:\n",
        "    - Multiple parallel convolutional layers (heads) with different kernel sizes.\n",
        "    - Each head extracts features at different scales.\n",
        "3. Concatenation Layer: Combine feature maps from all heads.\n",
        "4. Pooling Layer: Downsample the concatenated feature maps.\n",
        "5. Flatten Layer: Convert feature maps to 1D.\n",
        "6. Dense Layers: Output forecasted values.\n",
        "\n",
        "Multi-Headed CNN Components:\n",
        "\n",
        "1. Multiple Heads: Each head is a convolutional layer with a different kernel size, allowing the model to capture features at multiple scales.\n",
        "2. Kernel Sizes: Typically, small (e.g., 3), medium (e.g., 5), and large (e.g., 11) kernel sizes are used.\n",
        "3. Feature Concatenation: Combining features from all heads allows the model to leverage information from multiple scales.\n",
        "\n",
        "Theoretical Advantages:\n",
        "\n",
        "1. Multi-Scale Feature Extraction: Captures both short-term and long-term dependencies.\n",
        "2. Increased Receptive Field: Larger kernel sizes increase the receptive field, allowing the model to consider more historical values.\n",
        "3. Improved Feature Representation: Concatenating features from multiple heads provides a richer representation of the input data.\n",
        "\n",
        "Training Considerations:\n",
        "\n",
        "1. Loss Function: Mean Squared Error (MSE) or Mean Absolute Error (MAE) are common choices.\n",
        "2. Optimization Algorithm: Adam or SGD with momentum.\n",
        "3. Regularization Techniques: Dropout, L1/L2 regularization.\n",
        "\n",
        "Real-World Applications:\n",
        "\n",
        "1. Financial Forecasting: Predicting stock prices, currency exchange rates.\n",
        "2. Weather Forecasting: Predicting temperature, precipitation.\n",
        "3. Traffic Forecasting: Predicting traffic volume.\n",
        "\n",
        "Comparison to Other Models:\n",
        "\n",
        "1. Single-Headed CNN: Limited to a single scale, may miss important features.\n",
        "2. RNNs/LSTMs: May struggle with long-term dependencies, require more parameters.\n",
        "3. ARIMA: Limited to linear relationships.\n"
      ],
      "metadata": {
        "id": "tI9kPvCCeZ5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multivariate Multi-Headed 1D CNN example\n",
        "\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "\n",
        "# Split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequences)):\n",
        "        # Find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "\n",
        "        # Check if we are beyond the dataset\n",
        "        if end_ix > len(sequences):\n",
        "            break\n",
        "\n",
        "        # Gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "\n",
        "    return array(X), array(y)\n",
        "\n",
        "# Define input sequence\n",
        "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
        "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
        "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
        "\n",
        "# Convert to [rows, columns] structure\n",
        "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
        "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
        "out_seq = out_seq.reshape((len(out_seq), 1))\n",
        "\n",
        "# Horizontally stack columns\n",
        "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
        "\n",
        "# Choose a number of time steps\n",
        "n_steps = 3\n",
        "\n",
        "# Convert into input/output\n",
        "X, y = split_sequences(dataset, n_steps)\n",
        "\n",
        "# One time series per head\n",
        "n_features = 1\n",
        "\n",
        "# Separate input data\n",
        "X1 = X[:, :, 0].reshape(X.shape[0], X.shape[1], n_features)\n",
        "X2 = X[:, :, 1].reshape(X.shape[0], X.shape[1], n_features)\n",
        "\n",
        "# First input model\n",
        "visible1 = Input(shape=(n_steps, n_features))\n",
        "cnn1 = Conv1D(filters=64, kernel_size=2, activation='relu')(visible1)\n",
        "cnn1 = MaxPooling1D(pool_size=2)(cnn1)\n",
        "cnn1 = Flatten()(cnn1)\n",
        "\n",
        "# Second input model\n",
        "visible2 = Input(shape=(n_steps, n_features))\n",
        "cnn2 = Conv1D(filters=64, kernel_size=2, activation='relu')(visible2)\n",
        "cnn2 = MaxPooling1D(pool_size=2)(cnn2)\n",
        "cnn2 = Flatten()(cnn2)\n",
        "\n",
        "# Merge input models\n",
        "merge = concatenate([cnn1, cnn2])\n",
        "dense = Dense(50, activation='relu')(merge)\n",
        "output = Dense(1)(dense)\n",
        "\n",
        "model = Model(inputs=[visible1, visible2], outputs=output)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Fit model\n",
        "model.fit([X1, X2], y, epochs=1000, verbose=0)\n",
        "\n",
        "# Demonstrate prediction\n",
        "x_input = array([[80, 85], [90, 95], [100, 105]]) #6, 3,3 - input 3 features, 3 timestep\n",
        "x1 = x_input[:, 0].reshape((1, n_steps, n_features))\n",
        "x2 = x_input[:, 1].reshape((1, n_steps, n_features))\n",
        "yhat = model.predict([x1, x2], verbose=0)\n",
        "print(yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJcex11deada",
        "outputId": "ab5b4dbd-5cf0-4fc7-be4a-d0d5459f1ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[205.75526]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Parallel Series"
      ],
      "metadata": {
        "id": "wQWdQ8byjsx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "theoretical overview of Univariate Multiple Parallel Series forecasting:\n",
        "\n",
        "Problem Statement:\n",
        "\n",
        "Given a univariate time series dataset with multiple parallel series, forecast future values.\n",
        "\n",
        "Definition:\n",
        "\n",
        "- Univariate: Single variable or feature.\n",
        "- Multiple Parallel Series: Multiple time series with the same frequency and related to the same phenomenon.\n",
        "\n",
        "Example:\n",
        "\n",
        "- Daily temperature readings from multiple cities.\n",
        "- Monthly sales data from multiple stores.\n",
        "\n",
        "Theoretical Framework:\n",
        "\n",
        "1. Single-Model Approach: Train a single model on all parallel series.\n",
        "    - Advantages: Simplifies training and prediction.\n",
        "    - Disadvantages: May not capture series-specific patterns.\n",
        "2. Multi-Model Approach: Train separate models for each parallel series.\n",
        "    - Advantages: Captures series-specific patterns.\n",
        "    - Disadvantages: Increases model complexity and training time.\n",
        "3. Hybrid Approach: Combine single-model and multi-model approaches.\n",
        "    - Advantages: Balances simplicity and series-specific pattern capture.\n",
        "\n",
        "Architectures:\n",
        "\n",
        "1. Shared-Weights CNN: Single CNN with shared weights across all series.\n",
        "2. Separate-Weights CNN: Separate CNNs for each series.\n",
        "3. Hierarchical CNN: Shared CNN followed by series-specific CNNs.\n",
        "\n",
        "Training Strategies:\n",
        "\n",
        "1. Joint Training: Train all models simultaneously.\n",
        "2. Sequential Training: Train each model separately.\n",
        "3. Transfer Learning: Pre-train on one series, fine-tune on others.\n",
        "\n",
        "Evaluation Metrics:\n",
        "\n",
        "1. Mean Absolute Error (MAE).\n",
        "2. Mean Squared Error (MSE).\n",
        "3. Root Mean Squared Percentage Error (RMSPE).\n",
        "\n",
        "Real-World Applications:\n",
        "\n",
        "1. Climate Modeling: Forecast temperature, precipitation.\n",
        "2. Economics: Forecast GDP, inflation.\n",
        "3. Finance: Forecast stock prices.\n",
        "\n",
        "Comparison to Other Methods:\n",
        "\n",
        "1. ARIMA: Limited to linear relationships.\n",
        "2. LSTM: May struggle with long-term dependencies.\n",
        "3. Prophet: May not capture complex patterns.\n",
        "\n",
        "Theoretical Considerations:\n",
        "\n",
        "1. Series Correlation: Account for correlations between series.\n",
        "2. Series Heterogeneity: Handle varying series characteristics.\n",
        "3. Overfitting: Regularization techniques.\n",
        "\n",
        "Future Research Directions:\n",
        "\n",
        "1. Deep Learning Architectures: Explore novel architectures.\n",
        "2. Transfer Learning: Investigate pre-training strategies.\n",
        "3. Explainability: Develop techniques for interpreting forecasts.\n",
        "\n",
        "This theoretical overview provides a foundation for understanding univariate multiple parallel series forecasting."
      ],
      "metadata": {
        "id": "dVAopxJdkbOs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V_VhgX7QjvcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Parallel Serie"
      ],
      "metadata": {
        "id": "Uc0HqITHq80O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Multivariate Output 1D CNN Example\n",
        "\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "\n",
        "# Split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequences)):\n",
        "        # Find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "\n",
        "        # Check if we are beyond the dataset\n",
        "        if end_ix > len(sequences) - 1:\n",
        "            break\n",
        "\n",
        "        # Gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "\n",
        "    return array(X), array(y)\n",
        "\n",
        "# Define input sequence\n",
        "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
        "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
        "out_seq = array([in_seq1[i] + in_seq2[i] for i in range(len(in_seq1))])\n",
        "\n",
        "# Convert to [rows, columns] structure\n",
        "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
        "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
        "out_seq = out_seq.reshape((len(out_seq), 1))\n",
        "\n",
        "# Horizontally stack columns\n",
        "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
        "\n",
        "# Choose a number of time steps\n",
        "n_steps = 3\n",
        "\n",
        "# Convert into input/output\n",
        "X, y = split_sequences(dataset, n_steps)\n",
        "\n",
        "# The dataset knows the number of features, e.g. 3\n",
        "n_features = X.shape[2]\n",
        "\n",
        "# Define model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(n_features))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Fit model\n",
        "model.fit(X, y, epochs=3000, verbose=0)\n",
        "\n",
        "# Demonstrate prediction\n",
        "x_input = array([[70, 75, 145], [80, 85, 165], [90, 95, 185]])\n",
        "x_input = x_input.reshape((1, n_steps, n_features))\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZdi8x7XriRS",
        "outputId": "b5fa1931-a8a8-4af0-c5af-b6e213e479e3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[100.16192 105.37202 205.47534]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-R6CiCVqriPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Meta AI: Here's a theoretical explanation of a Univariate Multi-Output CNN model:\n",
        "\n",
        "Problem Statement:\n",
        "\n",
        "Given a univariate time series dataset, predict multiple future values simultaneously.\n",
        "\n",
        "Architecture:\n",
        "\n",
        "1. Input Layer: 1D sequence of univariate time series values.\n",
        "2. Convolutional Block:\n",
        "    - Multiple convolutional layers with different kernel sizes.\n",
        "    - Each layer extracts features at different scales.\n",
        "3. Flatten Layer: Convert feature maps to 1D.\n",
        "4. Dense Layers:\n",
        "    - Multiple dense layers with different output dimensions.\n",
        "    - Each layer predicts a specific future value.\n",
        "5. Output Layer: Multiple outputs, each corresponding to a predicted future value.\n",
        "\n",
        "Theoretical Advantages:\n",
        "\n",
        "1. Multi-Scale Feature Extraction: Captures both short-term and long-term dependencies.\n",
        "2. Simultaneous Prediction: Predicts multiple future values, reducing computational complexity.\n",
        "3. Shared Knowledge: Shared convolutional features across outputs improve overall performance.\n",
        "\n",
        "Key Components:\n",
        "\n",
        "1. Convolutional Layers: Extract local patterns and trends.\n",
        "2. Dilation: Increases receptive field, capturing longer-term dependencies.\n",
        "3. Residual Connections: Enhance feature propagation and training stability.\n",
        "4. Output Layers: Separate dense layers for each predicted value.\n",
        "\n",
        "Training Considerations:\n",
        "\n",
        "1. Loss Function: Combine losses for each output (e.g., MSE, MAE).\n",
        "2. Optimization Algorithm: Adam, SGD with momentum.\n",
        "3. Regularization Techniques: Dropout, L1/L2 regularization.\n",
        "\n",
        "Real-World Applications:\n",
        "\n",
        "1. Financial Forecasting: Predict stock prices, currency exchange rates.\n",
        "2. Weather Forecasting: Predict temperature, precipitation.\n",
        "3. Energy Demand Forecasting: Predict energy consumption.\n",
        "\n",
        "Comparison to Other Models:\n",
        "\n",
        "1. ARIMA: Limited to linear relationships.\n",
        "2. LSTM: May struggle with long-term dependencies.\n",
        "3. Prophet: May not capture complex patterns.\n",
        "\n",
        "Theoretical Considerations:\n",
        "\n",
        "1. Output Correlation: Account for correlations between predicted values.\n",
        "2. Output Heterogeneity: Handle varying output characteristics.\n",
        "3. Overfitting: Regularization techniques."
      ],
      "metadata": {
        "id": "2j25E7Bkq8x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multivariate Output 1D CNN Example\n",
        "\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "\n",
        "# Split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequences)):\n",
        "        # Find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "\n",
        "        # Check if we are beyond the dataset\n",
        "        if end_ix > len(sequences) - 1:\n",
        "            break\n",
        "\n",
        "        # Gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "\n",
        "    return array(X), array(y)\n",
        "\n",
        "# Define input sequence\n",
        "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
        "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
        "out_seq = array([in_seq1[i] + in_seq2[i] for i in range(len(in_seq1))])\n",
        "\n",
        "# Convert to [rows, columns] structure\n",
        "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
        "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
        "out_seq = out_seq.reshape((len(out_seq), 1))\n",
        "\n",
        "# Horizontally stack columns\n",
        "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
        "\n",
        "# Choose a number of time steps\n",
        "n_steps = 3\n",
        "\n",
        "# Convert into input/output\n",
        "X, y = split_sequences(dataset, n_steps)\n",
        "\n",
        "# The dataset knows the number of features, e.g. 3\n",
        "n_features = X.shape[2]\n",
        "\n",
        "# Separate output\n",
        "y1 = y[:, 0].reshape((y.shape[0], 1))\n",
        "y2 = y[:, 1].reshape((y.shape[0], 1))\n",
        "y3 = y[:, 2].reshape((y.shape[0], 1))\n",
        "\n",
        "# Define model\n",
        "visible = Input(shape=(n_steps, n_features))\n",
        "cnn = Conv1D(filters=64, kernel_size=2, activation='relu')(visible)\n",
        "cnn = MaxPooling1D(pool_size=2)(cnn)\n",
        "cnn = Flatten()(cnn)\n",
        "cnn = Dense(50, activation='relu')(cnn)\n",
        "\n",
        "# Define outputs\n",
        "output1 = Dense(1)(cnn)\n",
        "output2 = Dense(1)(cnn)\n",
        "output3 = Dense(1)(cnn)\n",
        "\n",
        "# Tie together\n",
        "model = Model(inputs=visible, outputs=[output1, output2, output3])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Fit model\n",
        "model.fit(X, [y1, y2, y3], epochs=2000, verbose=0)\n",
        "\n",
        "# Demonstrate prediction\n",
        "x_input = array([[70, 75, 145], [80, 85, 165], [90, 95, 185]])\n",
        "x_input = x_input.reshape((1, n_steps, n_features))\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J_94btctYtq",
        "outputId": "f89e8f57-b483-447b-c33c-dc5b72cc3385"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[101.35368]], dtype=float32), array([[107.16554]], dtype=float32), array([[208.35492]], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-step CNN models are a type of deep learning architecture used for time series forecasting and sequence prediction tasks. Here's a theoretical explanation:\n",
        "\n",
        "Architecture:\n",
        "\n",
        "1. Input Layer: 1D sequence of values.\n",
        "2. Convolutional Block:\n",
        "    - Multiple convolutional layers with different kernel sizes.\n",
        "    - Each layer extracts features at different scales.\n",
        "3. Flatten Layer: Convert feature maps to 1D.\n",
        "4. Dense Layers:\n",
        "    - Multiple dense layers with different output dimensions.\n",
        "    - Each layer predicts a specific future value.\n",
        "5. Output Layer: Multiple outputs, each corresponding to a predicted future value.\n",
        "\n",
        "Theory:\n",
        "\n",
        "1. Convolutional Neural Networks (CNNs): CNNs are designed to extract local patterns and trends in data.\n",
        "2. Multi-Step Prediction: Instead of predicting a single future value, multi-step models predict multiple future values.\n",
        "3. Sequence-to-Sequence (Seq2Seq) Modeling: Multi-step CNNs can be viewed as Seq2Seq models, where the input sequence is mapped to multiple output sequences.\n",
        "4. Temporal Hierarchies: Multi-step CNNs can capture temporal hierarchies by using multiple convolutional layers with different kernel sizes.\n",
        "\n",
        "Key Components:\n",
        "\n",
        "1. Convolutional Layers: Extract local patterns and trends.\n",
        "2. Dilation: Increases receptive field, capturing longer-term dependencies.\n",
        "3. Residual Connections: Enhance feature propagation and training stability.\n",
        "4. Output Layers: Separate dense layers for each predicted value.\n",
        "\n",
        "Training Considerations:\n",
        "\n",
        "1. Loss Function: Combine losses for each output (e.g., MSE, MAE).\n",
        "2. Optimization Algorithm: Adam, SGD with momentum.\n",
        "3. Regularization Techniques: Dropout, L1/L2 regularization.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "1. Improved Accuracy: Multi-step models can capture complex patterns.\n",
        "2. Increased Interpretability: Separate outputs provide insights into future values.\n",
        "3. Flexibility: Can be used for various sequence prediction tasks.\n",
        "\n",
        "Challenges:\n",
        "\n",
        "1. Increased Complexity: Multi-step models require more parameters.\n",
        "2. Overfitting: Regularization techniques are crucial.\n",
        "3. Computational Cost: Increased computational requirements.\n",
        "\n",
        "Real-World Applications:\n",
        "\n",
        "1. Financial Forecasting: Predict stock prices, currency exchange rates.\n",
        "2. Weather Forecasting: Predict temperature, precipitation.\n",
        "3. Energy Demand Forecasting: Predict energy consumption.\n",
        "\n",
        "Comparison to Other Models:\n",
        "\n",
        "1. ARIMA: Limited to linear relationships.\n",
        "2. LSTM: May struggle with long-term dependencies.\n",
        "3. Prophet: May not capture complex patterns."
      ],
      "metadata": {
        "id": "CDq-3zbcugJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Univariate Multi-Step Vector-Output 1D CNN Example\n",
        "\n",
        "from numpy import array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "\n",
        "# Split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequence)):\n",
        "        # Find the end of this pattern\n",
        "        end_ix = i + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out\n",
        "\n",
        "        # Check if we are beyond the sequence\n",
        "        if out_end_ix > len(sequence):\n",
        "            break\n",
        "\n",
        "        # Gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "\n",
        "    return array(X), array(y)\n",
        "\n",
        "# Define input sequence\n",
        "raw_seq = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
        "\n",
        "# Choose a number of time steps\n",
        "n_steps_in, n_steps_out = 3, 2\n",
        "\n",
        "# Split into samples\n",
        "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
        "\n",
        "# Reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "n_features = 1\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
        "\n",
        "# Define model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(n_steps_out))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Fit model\n",
        "model.fit(X, y, epochs=2000, verbose=0)\n",
        "\n",
        "# Demonstrate prediction\n",
        "x_input = array([70, 80, 90])\n",
        "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37MtI1uGu6LB",
        "outputId": "d23a9080-e3bf-4c5a-9b99-3a5a19c3a5d8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff744d28d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[102.53493 115.45038]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ARfewiyQwBqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multivarient Multi-Step CNN MODEL\n",
        "Here's a theoretical overview of 1D Multivariate Multi-Step CNN models:\n",
        "\n",
        "Architecture:\n",
        "\n",
        "1. Input Layer: 1D multivariate sequence (e.g., time series data with multiple variables).\n",
        "2. Convolutional Block:\n",
        "    - Multiple convolutional layers with different kernel sizes.\n",
        "    - Each layer extracts features from different scales.\n",
        "3. Flatten Layer: Convert feature maps to 1D.\n",
        "4. Dense Layers:\n",
        "    - Multiple dense layers with different output dimensions.\n",
        "    - Each layer predicts a specific future value.\n",
        "5. Output Layer: Multiple outputs, each corresponding to a predicted future value.\n",
        "\n",
        "Theory:\n",
        "\n",
        "1. Multivariate Input: Model takes multiple variables as input, capturing relationships between them.\n",
        "2. Multi-Step Prediction: Model predicts multiple future values, considering temporal dependencies.\n",
        "3. Convolutional Neural Networks (CNNs): Effective for extracting local patterns and trends.\n",
        "4. 1D CNNs: Suitable for sequential data, such as time series.\n",
        "5. Dilation: Increases receptive field, capturing longer-term dependencies.\n",
        "6. Residual Connections: Enhance feature propagation and training stability.\n",
        "\n",
        "Key Components:\n",
        "\n",
        "1. Convolutional Layers: Extract local patterns and trends.\n",
        "2. Max Pooling: Reduces spatial dimensions, retaining important features.\n",
        "3. Flatten Layer: Converts feature maps to 1D.\n",
        "4. Dense Layers: Predict future values.\n",
        "5. Output Layers: Separate dense layers for each predicted value.\n",
        "\n",
        "Training Considerations:\n",
        "\n",
        "1. Loss Function: Combine losses for each output (e.g., MSE, MAE).\n",
        "2. Optimization Algorithm: Adam, SGD with momentum.\n",
        "3. Regularization Techniques: Dropout, L1/L2 regularization.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "1. Improved Accuracy: Captures complex patterns and relationships.\n",
        "2. Increased Interpretability: Separate outputs provide insights into future values.\n",
        "3. Flexibility: Suitable for various multivariate time series forecasting tasks."
      ],
      "metadata": {
        "id": "LHlPIl_uwBtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Input Multi-Step Output models:\n",
        "\n",
        "Architecture:\n",
        "\n",
        "1. Multiple Input Layers: Each input layer processes a different input sequence.\n",
        "2. Concatenate or Merge Layer: Combines input features from different sequences.\n",
        "3. Convolutional Block:\n",
        "    - Multiple convolutional layers with different kernel sizes.\n",
        "    - Each layer extracts features from different scales.\n",
        "4. Flatten Layer: Convert feature maps to 1D.\n",
        "5. Dense Layers:\n",
        "    - Multiple dense layers with different output dimensions.\n",
        "    - Each layer predicts a specific future value.\n",
        "6. Output Layer: Multiple outputs, each corresponding to a predicted future value.\n",
        "\n",
        "Theory:\n",
        "\n",
        "1. Multiple Input: Model takes multiple input sequences, capturing relationships between them.\n",
        "2. Multi-Step Prediction: Model predicts multiple future values, considering temporal dependencies.\n",
        "3. Feature Fusion: Combines input features from different sequences to capture complex relationships.\n",
        "4. Convolutional Neural Networks (CNNs): Effective for extracting local patterns and trends.\n",
        "\n",
        "Key Components:\n",
        "\n",
        "1. Input Layers: Process different input sequences.\n",
        "2. Concatenate or Merge Layer: Combines input features.\n",
        "3. Convolutional Layers: Extract local patterns and trends.\n",
        "4. Dense Layers: Predict future values.\n",
        "5. Output Layers: Separate dense layers for each predicted value.\n",
        "\n",
        "Training Considerations:\n",
        "\n",
        "1. Loss Function: Combine losses for each output (e.g., MSE, MAE).\n",
        "2. Optimization Algorithm: Adam, SGD with momentum.\n",
        "3. Regularization Techniques: Dropout, L1/L2 regularization.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "1. Improved Accuracy: Captures complex relationships between input sequences.\n",
        "2. Increased Interpretability: Separate outputs provide insights into future values.\n",
        "3. Flexibility: Suitable for various multi-input multi-step forecasting tasks.\n",
        "\n",
        "Challenges:\n",
        "\n",
        "1. Increased Complexity: Requires careful architecture design.\n",
        "2. Overfitting: Regularization techniques are crucial.\n",
        "3. Computational Cost: Increased computational requirements."
      ],
      "metadata": {
        "id": "gCmJnWzBwYgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multivariate Multi-Step 1D CNN Example\n",
        "\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "\n",
        "# Split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequences)):\n",
        "        # Find the end of this pattern\n",
        "        end_ix = i + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out - 1\n",
        "\n",
        "        # Check if we are beyond the dataset\n",
        "        if out_end_ix > len(sequences):\n",
        "            break\n",
        "\n",
        "        # Gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "\n",
        "    return array(X), array(y)\n",
        "\n",
        "# Define input sequence\n",
        "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
        "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
        "out_seq = array([in_seq1[i] + in_seq2[i] for i in range(len(in_seq1))])\n",
        "\n",
        "# Convert to [rows, columns] structure\n",
        "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
        "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
        "out_seq = out_seq.reshape((len(out_seq), 1))\n",
        "\n",
        "# Horizontally stack columns\n",
        "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
        "\n",
        "# Choose a number of time steps\n",
        "n_steps_in, n_steps_out = 3, 2\n",
        "\n",
        "# Convert into input/output\n",
        "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
        "\n",
        "# The dataset knows the number of features, e.g. 2\n",
        "n_features = X.shape[2]\n",
        "\n",
        "# Define model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(n_steps_out))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Fit model\n",
        "model.fit(X, y, epochs=2000, verbose=0)\n",
        "\n",
        "# Demonstrate prediction\n",
        "x_input = array([[70, 75], [80, 85], [90, 95]])\n",
        "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3hkqH2ywHPE",
        "outputId": "43b099a6-171a-4dd8-9277-f80f7c91fec0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff744d29090> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[185.61382 207.25533]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Parallel Input and Multi-step Output"
      ],
      "metadata": {
        "id": "gKZJeR40w71K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Parallel Input and Multi-step Output models are a type of deep learning architecture designed for sequential data with multiple input variables and multiple output variables. Here's a theoretical overview:\n",
        "\n",
        "Architecture:\n",
        "\n",
        "1. Multiple Input Layers: Each input layer processes a different input sequence.\n",
        "2. Concatenate or Merge Layer: Combines input features from different sequences.\n",
        "3. Encoder Layer: Extracts relevant features from input sequences.\n",
        "4. Decoder Layer: Generates output sequences based on extracted features.\n",
        "5. Output Layer: Produces multiple output sequences.\n",
        "\n",
        "Theory:\n",
        "\n",
        "1. Multiple Input: Model takes multiple input sequences, capturing relationships between them.\n",
        "2. Multi-step Output: Model predicts multiple future values, considering temporal dependencies.\n",
        "3. Feature Fusion: Combines input features from different sequences to capture complex relationships.\n",
        "4. Sequence-to-Sequence (Seq2Seq) Modeling: Encoder-decoder architecture for sequential data.\n",
        "\n",
        "Key Components:\n",
        "\n",
        "1. Input Layers: Process different input sequences.\n",
        "2. Encoder Layer: Extracts relevant features from input sequences.\n",
        "3. Decoder Layer: Generates output sequences based on extracted features.\n",
        "4. Output Layer: Produces multiple output sequences.\n",
        "5. Activation Functions: Used in encoder and decoder layers (e.g., ReLU, LSTM).\n",
        "\n",
        "Training Considerations:\n",
        "\n",
        "1. Loss Function: Combine losses for each output sequence (e.g., MSE, MAE).\n",
        "2. Optimization Algorithm: Adam, SGD with momentum.\n",
        "3. Regularization Techniques: Dropout, L1/L2 regularization.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "1. Improved Accuracy: Captures complex relationships between input sequences.\n",
        "2. Increased Interpretability: Separate output sequences provide insights into future values.\n",
        "3. Flexibility: Suitable for various multi-input multi-step forecasting tasks.\n",
        "\n",
        "Challenges:\n",
        "\n",
        "1. Increased Complexity: Requires careful architecture design.\n",
        "2. Overfitting: Regularization techniques are crucial.\n",
        "3. Computational Cost: Increased computational requirements.\n",
        "\n",
        "Real-World Applications:\n",
        "\n",
        "1. Financial Forecasting: Predict stock prices, currency exchange rates.\n",
        "2. Weather Forecasting: Predict temperature, precipitation.\n",
        "3. Energy Demand Forecasting: Predict energy consumption.\n",
        "\n",
        "Example Architecture:\n",
        "\n",
        "\n",
        "                      +---------------+\n",
        "                      |  Input Layer  |\n",
        "                      +---------------+\n",
        "                             |\n",
        "                             |\n",
        "                             v\n",
        "                      +---------------+\n",
        "                      |  Encoder Layer  |\n",
        "                      |  (LSTM/Conv1D)  |\n",
        "                      +---------------+\n",
        "                             |\n",
        "                             |\n",
        "                             v\n",
        "                      +---------------+\n",
        "                      |  Decoder Layer  |\n",
        "                      |  (LSTM/Conv1D)  |\n",
        "                      +---------------+\n",
        "                             |\n",
        "                             |\n",
        "                             v\n",
        "                      +---------------+\n",
        "                      |  Output Layer  |\n",
        "                      |  (Dense)       |\n",
        "                      +---------------+\n",
        "\n"
      ],
      "metadata": {
        "id": "dBz37J5zxH9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Aq-hfpWpxH7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Multivariate Output Multi-Step 1D CNN Example\n",
        "\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "\n",
        "# Split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequences)):\n",
        "        # Find the end of this pattern\n",
        "        end_ix = i + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out\n",
        "\n",
        "        # Check if we are beyond the dataset\n",
        "        if out_end_ix > len(sequences):\n",
        "            break\n",
        "\n",
        "        # Gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "\n",
        "    return array(X), array(y)\n",
        "\n",
        "# Define input sequence\n",
        "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
        "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
        "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
        "\n",
        "# Convert to [rows, columns] structure\n",
        "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
        "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
        "out_seq = out_seq.reshape((len(out_seq), 1))\n",
        "\n",
        "# Horizontally stack columns\n",
        "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
        "\n",
        "# Choose a number of time steps\n",
        "n_steps_in, n_steps_out = 3, 2\n",
        "\n",
        "# Convert into input/output\n",
        "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
        "\n",
        "# Flatten output\n",
        "n_output = y.shape[1] * y.shape[2]\n",
        "y = y.reshape((y.shape[0], n_output))\n",
        "\n",
        "# The dataset knows the number of features\n",
        "n_features = X.shape[2]\n",
        "\n",
        "# Define model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(n_output))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Fit model\n",
        "model.fit(X, y, epochs=7000, verbose=0)\n",
        "\n",
        "# Demonstrate prediction\n",
        "x_input = array([[60, 65, 125], [70, 75, 145], [80, 85, 165]])\n",
        "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrIwy2Lhw8db",
        "outputId": "3816c1bc-6902-4d88-bcf6-8aff5e00607f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 90.56951  96.18933 187.33656 101.28927 106.82775 208.40642]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1uW0f4jCxt6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Changes made:\n",
        "\n",
        "1. Fixed indentation.\n",
        "2. Removed unnecessary comments.\n",
        "3. Renamed variables for clarity.\n",
        "4. Checked for consistency in variable naming conventions.\n",
        "\n",
        "This code defines a multivariate output multi-step 1D CNN model, trains it on a sample sequence, and demonstrates prediction. The model takes 3 input time steps and predicts 2 output time steps for 3 variables."
      ],
      "metadata": {
        "id": "Uj5Z-lCdxt9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_dnkeozAxucC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}